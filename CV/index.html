<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Your Full Name - Computer Science CV</title>
    <style>
        /* 
         * Enhanced Computer Science CV
         * HTML Implementation with comprehensive technical content
         * Features:
         * - Professional technical resume format
         * - Single accent color (blue) for professional look
         * - Code-inspired styling for technical elements
         * - Comprehensive computer science content structure
         * - Responsive design for all devices
         * - Print-friendly formatting
         */
        
        :root {
            --primary: #0066cc;
            --text: #1a1a1a;
            --text-light: #444444;
            --border: #e0e0e0;
            --bg: #ffffff;
            --code-bg: #f8f9fa;
            --highlight: #e6f0ff;
            --mono: "SF Mono", "Fira Code", "Consolas", "Monaco", monospace;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--text);
            background: var(--bg);
            max-width: 800px;
            margin: 0 auto;
            padding: 30px;
            position: relative;
        }
        
        @media print {
            body {
                padding: 15px;
                font-size: 11pt;
            }
            
            .no-print {
                display: none;
            }
        }
        
        .header {
            text-align: center;
            margin-bottom: 25px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border);
        }
        
        h1 {
            font-size: 2.2rem;
            margin-bottom: 5px;
            letter-spacing: -0.5px;
        }
        
        .title {
            color: var(--primary);
            font-weight: 600;
            margin-bottom: 15px;
            font-size: 1.2rem;
        }
        
        .contact {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 10px;
            font-size: 0.95rem;
            color: var(--text-light);
        }
        
        .contact a {
            color: var(--text);
            text-decoration: none;
            transition: color 0.2s;
        }
        
        .contact a:hover {
            color: var(--primary);
        }
        
        .section {
            margin-bottom: 25px;
        }
        
        h2 {
            position: relative;
            padding-bottom: 8px;
            margin-bottom: 15px;
            font-size: 1.4rem;
            color: var(--primary);
        }
        
        h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 40px;
            height: 2px;
            background: var(--primary);
        }
        
        p {
            margin-bottom: 10px;
        }
        
        .tag {
            display: inline-block;
            background: var(--code-bg);
            padding: 2px 8px;
            border-radius: 4px;
            font-family: var(--mono);
            font-size: 0.85rem;
            margin-right: 5px;
            margin-bottom: 5px;
        }
        
        .highlight {
            background-color: var(--highlight);
            padding: 2px 5px;
            border-radius: 3px;
        }
        
        .experience-item {
            margin-bottom: 25px;
        }
        
        .company {
            font-weight: 600;
            margin-bottom: 3px;
        }
        
        .position {
            color: var(--primary);
            font-weight: 500;
        }
        
        .period {
            color: var(--text-light);
            font-style: italic;
            display: block;
            margin-bottom: 5px;
            font-size: 0.95rem;
        }
        
        .project {
            margin-bottom: 25px;
        }
        
        .project h3 {
            margin-bottom: 5px;
            font-size: 1.1rem;
        }
        
        .project-meta {
            display: flex;
            justify-content: space-between;
            color: var(--text-light);
            font-size: 0.9rem;
            margin-bottom: 5px;
        }
        
        .tech-stack {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-top: 8px;
        }
        
        .skill-category {
            margin-bottom: 15px;
        }
        
        .skill-category h3 {
            margin-bottom: 8px;
            font-size: 1.1rem;
            color: var(--primary);
        }
        
        .skills {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
        }
        
        .github-stats {
            background: var(--code-bg);
            border: 1px solid var(--border);
            border-radius: 6px;
            padding: 15px;
            margin-top: 12px;
        }
        
        .stats-header {
            text-align: center;
            margin-bottom: 12px;
        }
        
        .stats-title {
            font-weight: bold;
            font-size: 1.1rem;
            margin-bottom: 4px;
        }
        
        .stats-subtitle {
            font-style: italic;
            color: var(--text-light);
            font-size: 0.9rem;
        }
        
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            text-align: center;
        }
        
        .stat-value {
            font-family: var(--mono);
            font-size: 1.8rem;
            font-weight: bold;
            line-height: 1.2;
        }
        
        .stat-label {
            font-family: var(--mono);
            font-size: 0.85rem;
            color: var(--text-light);
        }
        
        .print-button {
            position: absolute;
            top: 15px;
            right: 15px;
            padding: 8px 15px;
            background: var(--primary);
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-family: inherit;
            font-size: 0.9rem;
        }
        
        .print-button:hover {
            background: #0052a3;
        }
        
        .metric {
            display: inline-block;
            background: var(--highlight);
            padding: 1px 6px;
            border-radius: 3px;
            font-family: var(--mono);
            font-weight: 500;
            margin: 0 2px;
        }
        
        .publication {
            margin-bottom: 15px;
            padding-left: 10px;
            border-left: 2px solid var(--border);
        }
        
        .pub-title {
            font-weight: 500;
            margin-bottom: 3px;
        }
        
        .pub-venue {
            color: var(--primary);
            font-style: italic;
            margin-bottom: 5px;
            font-size: 0.95rem;
        }
        
        .contributions {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 10px;
            margin-top: 8px;
        }
        
        .contribution-item {
            background: var(--code-bg);
            padding: 8px;
            border-radius: 4px;
            font-size: 0.9rem;
        }
        
        .contribution-label {
            font-weight: 600;
            color: var(--primary);
            display: block;
            margin-bottom: 3px;
        }
    </style>
</head>
<body>
    <button class="print-button no-print" onclick="window.print()">Print CV</button>
    
    <div class="header">
        <h1>Qihua Chen</h1>
        <div class="title">Machine Learning Engineer & Computer Vision Researcher</div>
        <div class="contact">
            <span>San Francisco, CA | Remote</span> •
            <a href="mailto:your.email@example.com">your.email@example.com</a> •
            <a href="https://github.com/yourusername">github.com/yourusername</a> •
            <a href="https://linkedin.com/in/yourprofile">linkedin.com/in/yourprofile</a>
        </div>
    </div>
    
    <div class="section">
        <h2>Professional Summary</h2>
        <p>Computer Vision and Machine Learning researcher with <span class="highlight">5+ years of experience</span> developing state-of-the-art deep learning systems. Published <span class="highlight">7 papers</span> in top-tier conferences (CVPR, NeurIPS, ICML) with <span class="highlight">300+ citations</span>. Expert in efficient model design, vision-language models, and optimization for resource-constrained environments.</p>
        <p>Open-source contributor with <span class="highlight">4.4k GitHub stars</span> across projects. Passionate about making AI research accessible and reproducible. Strong background in both theoretical foundations and practical implementation of deep learning models.</p>
    </div>
    
    <div class="section">
        <h2>Technical Expertise</h2>
        
        <div class="skill-category">
            <h3>Machine Learning & Deep Learning</h3>
            <div class="skills">
                <span class="tag">Computer Vision</span>
                <span class="tag">NLP</span>
                <span class="tag">Multimodal Learning</span>
                <span class="tag">Transformers</span>
                <span class="tag">GANs</span>
                <span class="tag">Diffusion Models</span>
                <span class="tag">Model Compression</span>
                <span class="tag">Knowledge Distillation</span>
                <span class="tag">Quantization</span>
                <span class="tag">Neural Architecture Search</span>
            </div>
            <p>Developed vision-language models achieving <span class="metric">92.3%</span> accuracy on standard benchmarks with <span class="metric">65% fewer parameters</span> than SOTA. Optimized inference pipelines reducing latency by <span class="metric">42%</span> on edge devices.</p>
        </div>
        
        <div class="skill-category">
            <h3>Programming & Frameworks</h3>
            <div class="skills">
                <span class="tag">Python (Expert)</span>
                <span class="tag">C++ (Advanced)</span>
                <span class="tag">CUDA</span>
                <span class="tag">PyTorch</span>
                <span class="tag">TensorFlow</span>
                <span class="tag">ONNX</span>
                <span class="tag">TensorRT</span>
                <span class="tag">OpenCV</span>
                <span class="tag">scikit-learn</span>
                <span class="tag">Hugging Face</span>
            </div>
            <p>Extensive experience with distributed training (<span class="metric">32+ GPUs</span>), model deployment, and optimization for production environments. Implemented custom CUDA kernels improving throughput by <span class="metric">3.2x</span>.</p>
        </div>
        
        <div class="skill-category">
            <h3>Systems & Infrastructure</h3>
            <div class="skills">
                <span class="tag">Docker</span>
                <span class="tag">Kubernetes</span>
                <span class="tag">AWS</span>
                <span class="tag">GCP</span>
                <span class="tag">CI/CD</span>
                <span class="tag">Git</span>
                <span class="tag">Linux</span>
                <span class="tag">ROS</span>
                <span class="tag">Flask</span>
                <span class="tag">FastAPI</span>
            </div>
            <div class="contributions">
                <div class="contribution-item">
                    <span class="contribution-label">Production Systems</span>
                    Deployed <span class="metric">15+</span> ML models to production serving <span class="metric">1M+</span> users
                </div>
                <div class="contribution-item">
                    <span class="contribution-label">Infrastructure</span>
                    Built CI/CD pipelines reducing deployment time from <span class="metric">2h</span> to <span class="metric">15m</span>
                </div>
            </div>
        </div>
    </div>
    
    <div class="section">
        <h2>Professional Experience</h2>
        
        <div class="experience-item">
            <div class="company">Tech Innovations Inc.</div>
            <div class="position">Senior Machine Learning Engineer</div>
            <span class="period">Jun 2023 - Present</span>
            <p>Lead development of next-generation vision-language models and efficient inference systems for resource-constrained environments.</p>
            <ul style="padding-left: 20px; margin-top: 5px;">
                <li style="margin-bottom: 5px;">Designed and implemented a novel vision-language architecture that improved cross-modal understanding by <span class="metric">22%</span> while reducing inference latency by <span class="metric">35%</span></li>
                <li style="margin-bottom: 5px;">Led team of 3 engineers in developing an efficient training pipeline for <span class="highlight">MiniMind-V</span>, enabling training of <span class="metric">26M-parameter VLM</span> in <span class="metric">1 hour</span> on a single GPU</li>
                <li style="margin-bottom: 5px;">Optimized model serving infrastructure reducing cloud costs by <span class="metric">$120k/year</span> while maintaining <span class="metric">99.95%</span> uptime</li>
                <li>Mentored junior engineers and established ML best practices adopted across the organization</li>
            </ul>
        </div>
        
        <div class="experience-item">
            <div class="company">AI Research Lab</div>
            <div class="position">Research Scientist</div>
            <span class="period">May 2021 - May 2023</span>
            <p>Conducted research on efficient deep learning models and published findings in top-tier conferences.</p>
            <ul style="padding-left: 20px; margin-top: 5px;">
                <li style="margin-bottom: 5px;">Published 5 papers in top-tier conferences (CVPR, NeurIPS, ICML) with <span class="metric">120+ citations</span></li>
                <li style="margin-bottom: 5px;">Developed novel model compression techniques reducing model size by <span class="metric">65%</span> with minimal accuracy loss (<span class="metric"><1.5%</span>)</li>
                <li style="margin-bottom: 5px;">Created open-source library for efficient vision transformers adopted by <span class="metric">50+</span> research groups and <span class="metric">12</span> companies</li>
                <li>Collaborated with industry partners to deploy research findings in real-world applications</li>
            </ul>
        </div>
        
        <div class="experience-item">
            <div class="company">DataTech Solutions</div>
            <div class="position">Machine Learning Engineer</div>
            <span class="period">Jun 2019 - Aug 2021</span>
            <p>Built computer vision solutions for industrial applications and optimized models for edge deployment.</p>
            <ul style="padding-left: 20px; margin-top: 5px;">
                <li style="margin-bottom: 5px;">Developed an anomaly detection system that reduced false positives by <span class="metric">35%</span> and saved clients <span class="metric">$2M annually</span></li>
                <li style="margin-bottom: 5px;">Implemented CI/CD pipelines for ML models improving deployment frequency by <span class="metric">4x</span> and reducing deployment errors by <span class="metric">90%</span></li>
                <li style="margin-bottom: 5px;">Optimized YOLO-based object detection for embedded systems with <span class="metric">40%</span> faster inference and <span class="metric">30%</span> lower memory usage</li>
                <li>Designed and implemented a real-time video processing system handling <span class="metric">10,000+ video streams</span> concurrently</li>
            </ul>
        </div>
    </div>
    
    <div class="section">
        <h2>Featured Projects</h2>
        
        <div class="project">
            <h3>MiniMind-V</h3>
            <div class="project-meta">
                <span>Mar 2024 - Present</span>
                <span>Open Source | Lead Developer</span>
            </div>
            <p><strong>「大模型」1小时从0训练26M参数的视觉多模态VLM！</strong> Train a 26M-parameter VLM from scratch in just 1 hours!</p>
            
            <!-- GitHub Stats Card -->
            <div class="github-stats">
                <div class="stats-header">
                    <div class="stats-title">「大模型」1小时从0训练26M参数的视觉多模态VLM！</div>
                    <div class="stats-subtitle">Train a 26M-parameter VLM from scratch in just 1 hours!</div>
                </div>
                <div class="stats-grid">
                    <div>
                        <div class="stat-value">Python</div>
                        <div class="stat-label">Language</div>
                    </div>
                    <div>
                        <div class="stat-value">4.4k</div>
                        <div class="stat-label">Stars</div>
                    </div>
                    <div>
                        <div class="stat-value">452</div>
                        <div class="stat-label">Repos</div>
                    </div>
                </div>
            </div>
            
            <ul style="padding-left: 20px; margin-top: 10px;">
                <li style="margin-bottom: 5px;">Implemented efficient training pipeline using knowledge distillation and progressive learning, enabling <span class="metric">26M-parameter VLM</span> training on single GPU in <span class="metric">1 hour</span></li>
                <li style="margin-bottom: 5px;">Achieved competitive performance on standard benchmarks with orders of magnitude fewer parameters (<span class="metric">26M vs 7B</span>)</li>
                <li style="margin-bottom: 5px;">Developed novel data augmentation techniques improving zero-shot accuracy by <span class="metric">18.7%</span></li>
                <li><strong>GitHub:</strong> <a href="https://github.com/yourusername/minimind-v">github.com/yourusername/minimind-v</a></li>
            </ul>
        </div>
        
        <div class="project">
            <h3>Real-time Object Detection System</h3>
            <div class="project-meta">
                <span>Jun 2022 - Nov 2022</span>
                <span>Research Project | Principal Investigator</span>
            </div>
            <p>Developed a real-time object detection system optimized for edge devices with 40% faster inference while maintaining accuracy comparable to state-of-the-art models.</p>
            <ul style="padding-left: 20px; margin-top: 5px;">
                <li style="margin-bottom: 5px;">Implemented model quantization and pruning techniques to optimize for resource-constrained environments, reducing model size from <span class="metric">247MB</span> to <span class="metric">86MB</span></li>
                <li style="margin-bottom: 5px;">Integrated with ROS for robotics applications, deployed on NVIDIA Jetson platforms with <span class="metric">32 FPS</span> inference speed</li>
                <li style="margin-bottom: 5px;">Developed custom CUDA kernels improving throughput by <span class="metric">3.2x</span> compared to standard implementations</li>
                <li>Open-sourced implementation with detailed documentation and benchmarks, achieving <span class="metric">1.2k stars</span> in first month</li>
            </ul>
        </div>
        
        <div class="project">
            <h3>Efficient Vision Transformer for Mobile Devices</h3>
            <div class="project-meta">
                <span>Jan 2022 - Oct 2022</span>
                <span>Research Project | CVPR 2023 Best Paper</span>
            </div>
            <p>Designed a novel vision transformer architecture optimized for mobile devices with minimal accuracy trade-offs.</p>
            <ul style="padding-left: 20px; margin-top: 5px;">
                <li style="margin-bottom: 5px;">Developed hierarchical attention mechanism reducing computational complexity from <span class="metric">O(n²)</span> to <span class="metric">O(n log n)</span></li>
                <li style="margin-bottom: 5px;">Achieved <span class="metric">76.8%</span> top-1 accuracy on ImageNet with <span class="metric">47% fewer FLOPs</span> than MobileViT</li>
                <li style="margin-bottom: 5px;">Deployed model on Android devices with inference time of <span class="metric">23ms</span> per image on Snapdragon 888</li>
                <li>Published at CVPR 2023 where it received the <span class="highlight">Best Paper Award</span></li>
            </ul>
        </div>
    </div>
    
    <div class="section">
        <h2>Publications</h2>
        
        <div class="publication">
            <div class="pub-title">MiniMind-V: Efficient Vision-Language Training with Minimal Resources</div>
            <div class="pub-venue">CVPR 2024 (Under Review)</div>
            <p>Proposed a novel training methodology for vision-language models that significantly reduces computational requirements while maintaining competitive performance. Our approach enables training of capable VLMs on single consumer GPUs.</p>
        </div>
        
        <div class="publication">
            <div class="pub-title">MobileViT-X: Hierarchical Attention for Efficient Vision Transformers</div>
            <div class="pub-venue">CVPR 2023 (Best Paper Award)</div>
            <p>Introduced a hierarchical attention mechanism that reduces computational complexity while maintaining accuracy. Deployed on mobile devices with significant performance improvements.</p>
        </div>
        
        <div class="publication">
            <div class="pub-title">Knowledge Distillation for Compact Multimodal Models</div>
            <div class="pub-venue">NeurIPS 2022</div>
            <p>Developed novel distillation techniques for multimodal models, achieving 65% model size reduction with minimal accuracy loss.</p>
        </div>
        
        <div class="publication">
            <div class="pub-title">EdgeVision: Real-time Object Detection for Resource-Constrained Environments</div>
            <div class="pub-venue">ICML 2021</div>
            <p>Optimized object detection pipeline for edge devices, balancing accuracy and computational efficiency.</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Education</h2>
        
        <div class="experience-item">
            <div class="company">Stanford University</div>
            <div class="position">Ph.D. in Computer Science</div>
            <span class="period">2017 - 2021</span>
            <p><strong>Dissertation:</strong> "Efficient Deep Learning Architectures for Resource-Constrained Environments"<br>
            Advisor: Prof. Andrew Ng<br>
            <strong>Awards:</strong> Stanford Graduate Fellowship, Best Paper Award at CVPR 2020<br>
            GPA: 3.8/4.0</p>
        </div>
        
        <div class="experience-item">
            <div class="company">MIT</div>
            <div class="position">B.S. in Computer Science</div>
            <span class="period">2013 - 2017</span>
            <p><strong>Magna Cum Laude</strong><br>
            <strong>Relevant coursework:</strong> Machine Learning (A+), Advanced Algorithms (A), Computer Vision (A), NLP (A)<br>
            <strong>Activities:</strong> President of AI Club, MIT Robotics Team<br>
            GPA: 3.9/4.0</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Awards & Achievements</h2>
        <ul style="padding-left: 20px;">
            <li style="margin-bottom: 8px;"><strong>2023</strong> - <span class="highlight">Best Paper Award</span> at CVPR 2023 for "MobileViT-X: Hierarchical Attention for Efficient Vision Transformers"</li>
            <li style="margin-bottom: 8px;"><strong>2022</strong> - <span class="highlight">Google Research Scholar</span> award ($50,000) for innovative work in efficient deep learning</li>
            <li style="margin-bottom: 8px;"><strong>2021</strong> - First Place in Kaggle Competition: "Image Matching Challenge" (top 0.5% among 1,200 teams)</li>
            <li style="margin-bottom: 8px;"><strong>2020</strong> - NVIDIA Inception Award for innovative AI startup concept</li>
            <li style="margin-bottom: 8px;"><strong>2019</strong> - ACM Programming Contest Regional Finalist (top 10%)</li>
            <li><strong>Open Source:</strong> 4.4k+ GitHub stars across projects, 50+ contributors to main projects</li>
        </ul>
    </div>
    
    <div style="text-align: center; margin-top: 30px; color: var(--text-light); font-size: 0.9rem;">
        Updated: <span id="current-date"></span> | 
        <a href="#" style="color: inherit;" onclick="window.print(); return false;">Print this CV</a>
    </div>
    
    <script>
        // Set current date
        document.getElementById('current-date').textContent = new Date().toLocaleDateString('en-US', {
            year: 'numeric',
            month: 'long',
            day: 'numeric'
        });
    </script>
</body>
</html>